{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb42e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benkc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\benkc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ea3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "gossipcop_fake = pd.read_csv(\"../dataset/gossipcop_fake.csv\")\n",
    "gossipcop_fake['label'] = 1\n",
    "\n",
    "gossipcop_real = pd.read_csv(\"../dataset/gossipcop_real.csv\")\n",
    "gossipcop_real['label'] = 0\n",
    "\n",
    "politifact_fake = pd.read_csv(\"../dataset/politifact_fake.csv\")\n",
    "politifact_fake['label'] = 1\n",
    "\n",
    "politifact_real = pd.read_csv(\"../dataset/politifact_real.csv\")\n",
    "politifact_real['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbe2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./results/gossipcop\", exist_ok=True)\n",
    "os.makedirs(\"./results/politifact\", exist_ok=True)\n",
    "os.makedirs(\"./results/combined\", exist_ok=True)\n",
    "os.makedirs(\"./logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30c6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gossipcop = pd.concat([gossipcop_fake, gossipcop_real], ignore_index=True)\n",
    "df_politifact = pd.concat([politifact_fake, politifact_real], ignore_index=True)\n",
    "df_combined = pd.concat([gossipcop_fake, gossipcop_real, politifact_fake, politifact_real], ignore_index=True)\n",
    "\n",
    "df_gossipcop = df_gossipcop[['title', 'label']]\n",
    "df_politifact = df_politifact[['title', 'label']]\n",
    "df_combined = df_combined[['title', 'label']]\n",
    "\n",
    "# Save the combined dataset\n",
    "df_gossipcop.to_csv(\"gossipcop_dataset.csv\", index=False)\n",
    "df_politifact.to_csv(\"politifact_dataset.csv\", index=False)\n",
    "df_combined.to_csv(\"fakenews_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd8bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba55356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3814f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(df, tokenizer):\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['title'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "    val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b353ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, output_dir, model_name):\n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\"  # Disable reporting to avoid wandb etc.\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(f\"{output_dir}/{model_name}\")\n",
    "    tokenizer.save_pretrained(f\"{output_dir}/{model_name}\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Evaluation results for {model_name}:\")\n",
    "    print(eval_result)\n",
    "    \n",
    "    return model, eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa0ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing GossipCop datasets...\n",
      "Preparing PolitiFact datasets...\n",
      "Preparing combined datasets...\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "print(\"Preparing GossipCop datasets...\")\n",
    "gossipcop_train_dataset, gossipcop_val_dataset = prepare_datasets(df_gossipcop)\n",
    "\n",
    "print(\"Preparing PolitiFact datasets...\")\n",
    "politifact_train_dataset, politifact_val_dataset = prepare_datasets(df_politifact)\n",
    "\n",
    "print(\"Preparing combined datasets...\")\n",
    "combined_train_dataset, combined_val_dataset = prepare_datasets(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e57a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: gossipcop_model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6642' max='6642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6642/6642 3:17:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.381411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.476984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.553201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='554' max='554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [554/554 02:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for gossipcop_model:\n",
      "{'eval_loss': 0.3814108967781067, 'eval_runtime': 167.9603, 'eval_samples_per_second': 26.363, 'eval_steps_per_second': 3.298, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "gossipcop_model, gossipcop_results = train_model(\n",
    "    gossipcop_train_dataset, \n",
    "    gossipcop_val_dataset, \n",
    "    \"./results/gossipcop\", \n",
    "    \"gossipcop_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d121d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: politifact_model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 10:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.399250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.572944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.644151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for politifact_model:\n",
      "{'eval_loss': 0.3992496132850647, 'eval_runtime': 8.7794, 'eval_samples_per_second': 24.148, 'eval_steps_per_second': 3.075, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "politifact_model, politifact_results = train_model(\n",
    "    politifact_train_dataset, \n",
    "    politifact_val_dataset, \n",
    "    \"./results/politifact\", \n",
    "    \"politifact_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e12eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: combined_model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6960' max='6960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6960/6960 3:39:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.388743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.400736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.509931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='580' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [580/580 02:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for combined_model:\n",
      "{'eval_loss': 0.388743132352829, 'eval_runtime': 138.0045, 'eval_samples_per_second': 33.622, 'eval_steps_per_second': 4.203, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "combined_model, combined_results = train_model(\n",
    "    combined_train_dataset, \n",
    "    combined_val_dataset, \n",
    "    \"./results/combined\", \n",
    "    \"combined_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d07602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of training results:\n",
      "GossipCop Model: Loss = 0.3814\n",
      "PolitiFact Model: Loss = 0.3992\n",
      "Combined Model: Loss = 0.3887\n",
      "\n",
      "Training complete. Models saved in the following directories:\n",
      "- GossipCop model: ./results/gossipcop/gossipcop_model\n",
      "- PolitiFact model: ./results/politifact/politifact_model\n",
      "- Combined model: ./results/combined/combined_model\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"GossipCop Model\": gossipcop_results,\n",
    "    \"PolitiFact Model\": politifact_results,\n",
    "    \"Combined Model\": combined_results\n",
    "}\n",
    "\n",
    "print(\"\\nSummary of training results:\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}: Loss = {result['eval_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete. Models saved in the following directories:\")\n",
    "print(\"- GossipCop model: ./results/gossipcop/gossipcop_model\")\n",
    "print(\"- PolitiFact model: ./results/politifact/politifact_model\")\n",
    "print(\"- Combined model: ./results/combined/combined_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7eb6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23795351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f3dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benkc\\Computer System Secuity\\Group project\\FakeNewsNetWithModels\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer\n",
    "\n",
    "politifact_model = BertForSequenceClassification.from_pretrained(\"./results/politifact/politifact_model\")\n",
    "politifact_tokenizer = BertTokenizer.from_pretrained(\"./results/politifact/politifact_model\")\n",
    "\n",
    "Gossipcop_model = BertForSequenceClassification.from_pretrained(\"./results/gossipcop/gossipcop_model\")\n",
    "Gossipcop_tokenizer = BertTokenizer.from_pretrained(\"./results/gossipcop/gossipcop_model\")\n",
    "\n",
    "combined_model = BertForSequenceClassification.from_pretrained(\"./results/combined/combined_model\")\n",
    "combined_tokenizer = BertTokenizer.from_pretrained(\"./results/combined/combined_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a636d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "politifact_df = pd.read_csv(\"../dataset/politifact_dataset.csv\")\n",
    "_, val_dataset = prepare_datasets(politifact_df, politifact_tokenizer)\n",
    "\n",
    "gossipcop_df = pd.read_csv(\"../dataset/gossipcop_dataset.csv\")\n",
    "_, val_dataset = prepare_datasets(gossipcop_df, Gossipcop_tokenizer)\n",
    "\n",
    "combined_df = pd.read_csv(\"../dataset/combined.csv\")\n",
    "_, val_dataset = prepare_datasets(combined_df, combined_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f437a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benkc\\AppData\\Local\\Temp\\ipykernel_9452\\156325022.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  politifact_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.2959051724137931, 'f1': 0.40263302249040045, 'precision': 0.2548021291367739, 'recall': 0.9590592334494773}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benkc\\AppData\\Local\\Temp\\ipykernel_9452\\156325022.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  combined_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8424568965517242, 'f1': 0.6663623916020082, 'precision': 0.6999041227229147, 'recall': 0.6358885017421603}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benkc\\AppData\\Local\\Temp\\ipykernel_9452\\156325022.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  Gossipcop_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8752155172413794, 'f1': 0.7106446776611695, 'precision': 0.8335287221570926, 'recall': 0.6193379790940766}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "politifact_trainer = Trainer(\n",
    "    model=politifact_model,\n",
    "    tokenizer=politifact_tokenizer\n",
    ")\n",
    "\n",
    "predictions = politifact_trainer.predict(val_dataset)\n",
    "metrics = compute_metrics(predictions)\n",
    "print(metrics)\n",
    "\n",
    "combined_trainer = Trainer(\n",
    "    model=combined_model,\n",
    "    tokenizer=combined_tokenizer\n",
    ")\n",
    "\n",
    "predictions = combined_trainer.predict(val_dataset)\n",
    "metrics = compute_metrics(predictions)\n",
    "print(metrics)\n",
    "\n",
    "Gossipcop_trainer = Trainer(\n",
    "    model=Gossipcop_model,\n",
    "    tokenizer=Gossipcop_tokenizer\n",
    ")\n",
    "\n",
    "predictions = Gossipcop_trainer.predict(val_dataset)\n",
    "metrics = compute_metrics(predictions)\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
